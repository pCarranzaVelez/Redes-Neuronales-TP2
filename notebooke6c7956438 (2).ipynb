{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pylab as plt","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-07-04T18:44:40.400965Z","iopub.execute_input":"2022-07-04T18:44:40.401963Z","iopub.status.idle":"2022-07-04T18:44:40.409195Z","shell.execute_reply.started":"2022-07-04T18:44:40.401921Z","shell.execute_reply":"2022-07-04T18:44:40.408113Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"input_path = '../input/tgs-salt-identification-challenge/'\ntrain_path = input_path + 'train'\ntest_path = input_path + 'test'\n\ntrain_df = pd.read_csv(input_path + 'train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:44:42.149466Z","iopub.execute_input":"2022-07-04T18:44:42.149818Z","iopub.status.idle":"2022-07-04T18:44:42.181916Z","shell.execute_reply.started":"2022-07-04T18:44:42.149788Z","shell.execute_reply":"2022-07-04T18:44:42.181047Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n!pip install image-dataset-viz\nfrom image_dataset_viz import render_datapoint","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:44:43.474024Z","iopub.execute_input":"2022-07-04T18:44:43.474903Z","iopub.status.idle":"2022-07-04T18:44:54.844396Z","shell.execute_reply.started":"2022-07-04T18:44:43.474865Z","shell.execute_reply":"2022-07-04T18:44:54.843293Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport zipfile\n\nDataset = train_path\n\ntrain_path = './train_path'\nif not os.path.exists(train_path):\n    os.mkdir(train_path)\n\nwith zipfile.ZipFile(Dataset+\".zip\",\"r\") as z:\n    z.extractall(train_path)\n    \nDataset = test_path\n\ntest_path = './test_path'\nif not os.path.exists(test_path):\n    os.mkdir(test_path)\n\nwith zipfile.ZipFile(Dataset+\".zip\",\"r\") as z:\n    z.extractall(test_path)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:03.873984Z","iopub.execute_input":"2022-07-04T18:45:03.874376Z","iopub.status.idle":"2022-07-04T18:45:10.840946Z","shell.execute_reply.started":"2022-07-04T18:45:03.874341Z","shell.execute_reply":"2022-07-04T18:45:10.840016Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"image='0a0814464f'\ndef read_image(data_id, is_train=True):    \n    path = train_path if is_train else test_path\n    path = (path + \"/images\" + \"/{}.png\".format(data_id))\n    img = Image.open(path)\n    img = img.convert('RGB')\n    return img\n    \ndef read_mask(data_id, is_train=True):\n    path = train_path if is_train else test_path\n    path = (path + \"/masks\" + \"/{}.png\".format(data_id))    \n    img = Image.open(path)\n    bk = Image.new('L', size=img.size)\n    g = Image.merge('RGB', (bk, img.convert('L'), bk))\n    return g","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:19.175605Z","iopub.execute_input":"2022-07-04T18:45:19.175948Z","iopub.status.idle":"2022-07-04T18:45:19.184944Z","shell.execute_reply.started":"2022-07-04T18:45:19.175919Z","shell.execute_reply":"2022-07-04T18:45:19.183237Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"img = read_image(image)\nmask = read_mask(image)\nrimg = render_datapoint(img, mask, blend_alpha=0.3)\nprint(rimg.size)\nrimg","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:21.110034Z","iopub.execute_input":"2022-07-04T18:45:21.110689Z","iopub.status.idle":"2022-07-04T18:45:21.137948Z","shell.execute_reply.started":"2022-07-04T18:45:21.110650Z","shell.execute_reply":"2022-07-04T18:45:21.137040Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#export dataset to few images to easily visualize them\ndata_ids = train_df['id'].values.tolist()\n\n\nfrom image_dataset_viz import DatasetExporter\n\n\nde = DatasetExporter(read_image, read_mask, blend_alpha=0.3, n_cols=20, max_output_img_size=(100, 100))\nde.export(data_ids, data_ids, \"train_dataset_viz\")","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:24.755078Z","iopub.execute_input":"2022-07-04T18:45:24.755631Z","iopub.status.idle":"2022-07-04T18:45:46.384918Z","shell.execute_reply.started":"2022-07-04T18:45:24.755594Z","shell.execute_reply":"2022-07-04T18:45:46.383841Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Implementaci√≥n de U-NET","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\n#plt.style.use('seaborn-whimport seaborn as sns ie's\n#pip install seaborn\nimport seaborn as sns\nsns.set_style(\"white\")\n\n#pip install sklearn\n#pip install tensorflow\n#pip install scikit-image\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom tensorflow.keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\n","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:46.386904Z","iopub.execute_input":"2022-07-04T18:45:46.387367Z","iopub.status.idle":"2022-07-04T18:45:52.050298Z","shell.execute_reply.started":"2022-07-04T18:45:46.387326Z","shell.execute_reply":"2022-07-04T18:45:52.049316Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', anti_aliasing=True,preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant',anti_aliasing=True, preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:52.051508Z","iopub.execute_input":"2022-07-04T18:45:52.052186Z","iopub.status.idle":"2022-07-04T18:45:52.058044Z","shell.execute_reply.started":"2022-07-04T18:45:52.052148Z","shell.execute_reply":"2022-07-04T18:45:52.057266Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Loading of training/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train.","metadata":{}},{"cell_type":"code","source":"input_path = '../input/tgs-salt-identification-challenge/'\n\ntrain_df = pd.read_csv(input_path + 'train.csv', index_col='id', usecols=[0])\ndepths_df = pd.read_csv(input_path + 'depths.csv', index_col='id')\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\ntrain_df.reset_index()\ntest_df.reset_index()\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:52.060420Z","iopub.execute_input":"2022-07-04T18:45:52.060984Z","iopub.status.idle":"2022-07-04T18:45:52.151591Z","shell.execute_reply.started":"2022-07-04T18:45:52.060931Z","shell.execute_reply":"2022-07-04T18:45:52.150720Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:52.154041Z","iopub.execute_input":"2022-07-04T18:45:52.154663Z","iopub.status.idle":"2022-07-04T18:45:52.163014Z","shell.execute_reply.started":"2022-07-04T18:45:52.154627Z","shell.execute_reply":"2022-07-04T18:45:52.162053Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"img_path = train_path + \"/images/{}.png\"\ntrain_df[\"images\"] = [(np.array(load_img(img_path.format(idx), color_mode = \"grayscale\"))+20*np.random.normal(size=[101,101])) / 255 for idx in train_df.index]","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:52.164604Z","iopub.execute_input":"2022-07-04T18:45:52.164949Z","iopub.status.idle":"2022-07-04T18:45:56.390577Z","shell.execute_reply.started":"2022-07-04T18:45:52.164914Z","shell.execute_reply":"2022-07-04T18:45:56.389616Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"img_path = train_path + \"/masks/{}.png\"\ntrain_df[\"masks\"] = [np.array(load_img(img_path.format(idx), color_mode = \"grayscale\")) / 255 for idx in train_df.index]","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:56.392129Z","iopub.execute_input":"2022-07-04T18:45:56.392496Z","iopub.status.idle":"2022-07-04T18:45:57.481914Z","shell.execute_reply.started":"2022-07-04T18:45:56.392457Z","shell.execute_reply":"2022-07-04T18:45:57.480910Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Calculating the salt coverage and salt coverage classes","metadata":{}},{"cell_type":"markdown","source":"Counting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only. Plotting the distribution of coverages and coverage classes, and the class against the raw coverage.","metadata":{}},{"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / (pow(img_size_ori, 2)*257)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:57.483307Z","iopub.execute_input":"2022-07-04T18:45:57.483870Z","iopub.status.idle":"2022-07-04T18:45:57.544405Z","shell.execute_reply.started":"2022-07-04T18:45:57.483829Z","shell.execute_reply":"2022-07-04T18:45:57.543514Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \n \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n\nprint(train_df.coverage[0])","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:57.545689Z","iopub.execute_input":"2022-07-04T18:45:57.546136Z","iopub.status.idle":"2022-07-04T18:45:57.561046Z","shell.execute_reply.started":"2022-07-04T18:45:57.546100Z","shell.execute_reply":"2022-07-04T18:45:57.560052Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:57.565111Z","iopub.execute_input":"2022-07-04T18:45:57.565359Z","iopub.status.idle":"2022-07-04T18:45:58.667679Z","shell.execute_reply.started":"2022-07-04T18:45:57.565337Z","shell.execute_reply":"2022-07-04T18:45:58.666784Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.histplot(train_df.coverage, kde=False, ax=axs[0])\nsns.histplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:58.668967Z","iopub.execute_input":"2022-07-04T18:45:58.669444Z","iopub.status.idle":"2022-07-04T18:45:59.108824Z","shell.execute_reply.started":"2022-07-04T18:45:58.669387Z","shell.execute_reply":"2022-07-04T18:45:59.107934Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plt.scatter(train_df.coverage, train_df.coverage_class)\nplt.xlabel(\"Coverage\")\nplt.ylabel(\"Coverage class\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:59.110169Z","iopub.execute_input":"2022-07-04T18:45:59.110616Z","iopub.status.idle":"2022-07-04T18:45:59.326811Z","shell.execute_reply.started":"2022-07-04T18:45:59.110575Z","shell.execute_reply":"2022-07-04T18:45:59.325884Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Plotting the depth distributions","metadata":{}},{"cell_type":"markdown","source":"Separatelty plotting the depth distributions for the training and the testing data.","metadata":{}},{"cell_type":"code","source":"sns.displot(train_df.z, label=\"Train\")\nsns.displot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:45:59.328182Z","iopub.execute_input":"2022-07-04T18:45:59.329110Z","iopub.status.idle":"2022-07-04T18:46:00.076949Z","shell.execute_reply.started":"2022-07-04T18:45:59.329070Z","shell.execute_reply":"2022-07-04T18:46:00.076040Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Show some example images","metadata":{}},{"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:00.078469Z","iopub.execute_input":"2022-07-04T18:46:00.079083Z","iopub.status.idle":"2022-07-04T18:46:04.635887Z","shell.execute_reply.started":"2022-07-04T18:46:00.079043Z","shell.execute_reply":"2022-07-04T18:46:04.634958Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Create train/validation split stratified by salt coverage","metadata":{}},{"cell_type":"markdown","source":"Using the salt coverage as a stratification criterion. Also show an image to check for correct upsampling.","metadata":{}},{"cell_type":"code","source":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:04.638820Z","iopub.execute_input":"2022-07-04T18:46:04.639199Z","iopub.status.idle":"2022-07-04T18:46:21.556875Z","shell.execute_reply.started":"2022-07-04T18:46:04.639158Z","shell.execute_reply":"2022-07-04T18:46:21.555776Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:21.558216Z","iopub.execute_input":"2022-07-04T18:46:21.559213Z","iopub.status.idle":"2022-07-04T18:46:21.567133Z","shell.execute_reply.started":"2022-07-04T18:46:21.559161Z","shell.execute_reply":"2022-07-04T18:46:21.566178Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:21.568482Z","iopub.execute_input":"2022-07-04T18:46:21.568970Z","iopub.status.idle":"2022-07-04T18:46:21.964810Z","shell.execute_reply.started":"2022-07-04T18:46:21.568935Z","shell.execute_reply":"2022-07-04T18:46:21.963929Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Build Model","metadata":{}},{"cell_type":"code","source":"def conv_block(m, dim, acti, bn, res, do=0):\n    n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n    n = BatchNormalization()(n) if bn else n\n    n = Dropout(do)(n) if do else n\n    n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n    n = BatchNormalization()(n) if bn else n\n    return Concatenate()([m, n]) if res else n\n\ndef level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n    if depth > 0:\n        n = conv_block(m, dim, acti, bn, res)\n        m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n        m = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n        if up:\n            m = UpSampling2D()(m)\n            m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n        else:\n            m = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n        n = Concatenate()([n, m])\n        m = conv_block(n, dim, acti, bn, res)\n    else:\n        m = conv_block(m, dim, acti, bn, res, do)\n    return m\n\ndef UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n         dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n    i = Input(shape=img_shape)\n    o = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n    o = Conv2D(out_ch, 1, activation='sigmoid')(o)\n    return Model(inputs=i, outputs=o)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:21.967828Z","iopub.execute_input":"2022-07-04T18:46:21.968472Z","iopub.status.idle":"2022-07-04T18:46:21.983233Z","shell.execute_reply.started":"2022-07-04T18:46:21.968429Z","shell.execute_reply":"2022-07-04T18:46:21.982057Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = UNet((img_size_target,img_size_target,1),start_ch=32,depth=5,batchnorm=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:21.984620Z","iopub.execute_input":"2022-07-04T18:46:21.985098Z","iopub.status.idle":"2022-07-04T18:46:24.889726Z","shell.execute_reply.started":"2022-07-04T18:46:21.985063Z","shell.execute_reply":"2022-07-04T18:46:24.888738Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:24.891080Z","iopub.execute_input":"2022-07-04T18:46:24.891430Z","iopub.status.idle":"2022-07-04T18:46:24.919543Z","shell.execute_reply.started":"2022-07-04T18:46:24.891397Z","shell.execute_reply":"2022-07-04T18:46:24.918365Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:24.921220Z","iopub.execute_input":"2022-07-04T18:46:24.921647Z","iopub.status.idle":"2022-07-04T18:46:25.820763Z","shell.execute_reply.started":"2022-07-04T18:46:24.921612Z","shell.execute_reply":"2022-07-04T18:46:25.819764Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:25.822528Z","iopub.execute_input":"2022-07-04T18:46:25.822902Z","iopub.status.idle":"2022-07-04T18:46:28.021868Z","shell.execute_reply.started":"2022-07-04T18:46:25.822866Z","shell.execute_reply":"2022-07-04T18:46:28.020958Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import keras.backend as K\nearly_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n\ndef iou_coef(y_true, y_pred, smooth=1):\n#    \"\"\"\n#    IoU = (|X & Y|)/ (|X or Y|)\n#    \"\"\"\n    intersection = K.sum(K.abs(y_true * y_pred))\n    union = K.sum((y_true) + K.sum(y_pred)) - intersection\n    return ((intersection + smooth)/(union + smooth))\n\ndef iou_coef_loss(y_true, y_pred):\n    return -iou_coef(y_true, y_pred)\n                  \nmodel.compile(loss='mse', optimizer=\"nadam\", metrics=[\"accuracy\"])\nepochs = 200\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=(x_valid, y_valid), \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr],shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:46:28.023452Z","iopub.execute_input":"2022-07-04T18:46:28.024062Z","iopub.status.idle":"2022-07-04T18:54:52.250895Z","shell.execute_reply.started":"2022-07-04T18:46:28.024026Z","shell.execute_reply":"2022-07-04T18:54:52.249967Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_loss.legend()\nax_acc.plot(history.epoch, history.history[\"accuracy\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation accuracy\")\nax_acc.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:54:52.255344Z","iopub.execute_input":"2022-07-04T18:54:52.256040Z","iopub.status.idle":"2022-07-04T18:54:58.713011Z","shell.execute_reply.started":"2022-07-04T18:54:52.255989Z","shell.execute_reply":"2022-07-04T18:54:58.711961Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model = load_model(\"./keras.model\")","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:54:58.714293Z","iopub.execute_input":"2022-07-04T18:54:58.714666Z","iopub.status.idle":"2022-07-04T18:55:03.860740Z","shell.execute_reply.started":"2022-07-04T18:54:58.714628Z","shell.execute_reply":"2022-07-04T18:55:03.859432Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions.","metadata":{}},{"cell_type":"code","source":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])\nmax_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:55:03.887244Z","iopub.execute_input":"2022-07-04T18:55:03.887655Z","iopub.status.idle":"2022-07-04T18:55:16.124738Z","shell.execute_reply.started":"2022-07-04T18:55:03.887615Z","shell.execute_reply":"2022-07-04T18:55:16.123671Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Scoring\nScore the model and do a threshold optimization by the best IoU.","metadata":{}},{"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:55:16.126086Z","iopub.execute_input":"2022-07-04T18:55:16.127283Z","iopub.status.idle":"2022-07-04T18:55:16.145671Z","shell.execute_reply.started":"2022-07-04T18:55:16.127236Z","shell.execute_reply":"2022-07-04T18:55:16.144820Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in thresholds])\nthreshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:55:16.146733Z","iopub.execute_input":"2022-07-04T18:55:16.147461Z","iopub.status.idle":"2022-07-04T18:56:27.430832Z","shell.execute_reply.started":"2022-07-04T18:55:16.147414Z","shell.execute_reply":"2022-07-04T18:56:27.429881Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:56:27.432460Z","iopub.execute_input":"2022-07-04T18:56:27.432819Z","iopub.status.idle":"2022-07-04T18:56:27.667792Z","shell.execute_reply.started":"2022-07-04T18:56:27.432784Z","shell.execute_reply":"2022-07-04T18:56:27.666906Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Another sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold.","metadata":{}},{"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:56:27.669327Z","iopub.execute_input":"2022-07-04T18:56:27.669679Z","iopub.status.idle":"2022-07-04T18:56:33.268570Z","shell.execute_reply.started":"2022-07-04T18:56:27.669643Z","shell.execute_reply":"2022-07-04T18:56:33.267017Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Submission\nLoad, predict and submit the test image predictions.","metadata":{}},{"cell_type":"code","source":"# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:56:33.270250Z","iopub.execute_input":"2022-07-04T18:56:33.270870Z","iopub.status.idle":"2022-07-04T18:56:33.281056Z","shell.execute_reply.started":"2022-07-04T18:56:33.270834Z","shell.execute_reply":"2022-07-04T18:56:33.279984Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"img_path = test_path + \"/masks/{}.png\"\ntrain_df[\"masks\"] = [np.array(load_img(img_path.format(idx), color_mode = \"grayscale\")) / 255 for idx in test_df.index]","metadata":{"execution":{"iopub.status.busy":"2022-07-04T18:56:44.925791Z","iopub.execute_input":"2022-07-04T18:56:44.927081Z","iopub.status.idle":"2022-07-04T18:56:45.202212Z","shell.execute_reply.started":"2022-07-04T18:56:44.927030Z","shell.execute_reply":"2022-07-04T18:56:45.200121Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"preds_test = model.predict(x_test)\npred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(test_df.index.values)}\nsub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}